# EdgeRunner

**Real-Time Edge AI Vision System on NVIDIA Jetson Orin Nano**

EdgeRunner is a dual-thread computer vision system that runs YOLO object detection and Vision Language Model (VLM) inference entirely on edge hardware â€” no cloud, no latency, no connectivity dependency.

Built by [Nosis AI](https://github.com/nosis-ai) for industrial safety and autonomous monitoring applications.

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EdgeRunner Runtime                        â”‚
â”‚                  (Jetson Orin Nano 8GB)                      â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Thread 1: "The Reflex" â”‚  â”‚  Thread 2: "The Brain"    â”‚ â”‚
â”‚  â”‚       (30 FPS)          â”‚  â”‚       (0.1 Hz)            â”‚ â”‚
â”‚  â”‚                         â”‚  â”‚                           â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚ YOLO TensorRT   â”‚   â”‚  â”‚  â”‚ VLM (Qwen3-VL 2B)  â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ + ByteTrack     â”‚   â”‚  â”‚  â”‚ + JSON Constrained  â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚  â”‚   Decoding (GBNF)   â”‚  â”‚ â”‚
â”‚  â”‚           â”‚            â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â”‚           â–¼            â”‚  â”‚             â–¼             â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚  SLA Metrics    â”‚â—„â”€â”€â”¼â”€â”€â”¼â”€â”€â”‚  Decision Contract  â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  (FPS, P50/P95) â”‚   â”‚  â”‚  â”‚  (Pydantic Schema)  â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Dashboard: MJPEG Stream + SSE Events + SLA Metrics     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Performance

| Metric | Value | Hardware |
|--------|-------|----------|
| Detection FPS | **28+ FPS** | YOLO TensorRT FP16 + ByteTrack |
| VLM Latency | **8-9s** | Qwen3-VL 2B, constrained decode |
| JSON Parse Rate | **100%** | Grammar-enforced (GBNF) |
| VLM Fallback Rate | **0%** | Over 30+ min stability runs |
| Memory Usage | **6.5GB / 8GB** | Stable, no OOM |
| Power Draw | **~25W** | MAXN mode |

---

## Key Innovation: Constrained Decoding

Traditional VLM pipelines rely on prompt engineering and hope the output parses correctly. EdgeRunner uses **decode-time grammar constraints** via llama.cpp's GBNF support to guarantee 100% valid JSON output:

```python
# Traditional approach (unreliable)
response = vlm.generate("Output JSON: {detected: true/false}")
result = json.loads(response)  # May fail

# EdgeRunner approach (guaranteed)
response_format = {
    "type": "json_schema",
    "json_schema": {"schema": DecisionContract.model_json_schema()}
}
# VLM can ONLY output tokens that form valid JSON matching the schema
```

The decision contract is defined as a Pydantic v2 model with strict validation, ensuring every VLM output is machine-actionable.

---

## Project Structure

```
edge-runner/
â”œâ”€â”€ src/                      # Core source
â”‚   â”œâ”€â”€ main.py               # Triple-thread orchestrator + dashboard
â”‚   â”œâ”€â”€ cv_loop.py            # YOLO TensorRT + ByteTrack detection loop
â”‚   â”œâ”€â”€ vlm_client.py         # VLM client with constrained decoding
â”‚   â”œâ”€â”€ vlm_worker.py         # Async VLM inference worker
â”‚   â”œâ”€â”€ machina_schema.py     # Pydantic decision contract schema
â”‚   â”œâ”€â”€ state_machine.py      # FSM for system control states
â”‚   â”œâ”€â”€ state.py              # Thread-safe shared state
â”‚   â”œâ”€â”€ server.py             # FastAPI backend + MJPEG stream
â”‚   â”œâ”€â”€ alert_manager.py      # Event-driven alert system
â”‚   â”œâ”€â”€ proximity_detector.py # Ground-plane distance estimation
â”‚   â”œâ”€â”€ bbox_smoother.py      # Kalman-style bounding box stabilizer
â”‚   â”œâ”€â”€ ttc.py                # Time-to-contact predictor
â”‚   â”œâ”€â”€ zone_manager.py       # ROI zone management
â”‚   â”œâ”€â”€ zone_calibrator.py    # VLM-assisted zone calibration
â”‚   â””â”€â”€ ...
â”œâ”€â”€ tools/                    # Utilities
â”‚   â”œâ”€â”€ stability_test.py     # 30-min+ soak test with metrics
â”‚   â”œâ”€â”€ camera_check.py       # Camera diagnostics + focus tool
â”‚   â”œâ”€â”€ calibrate_floor.py    # Ground plane calibration
â”‚   â””â”€â”€ safety_audit_report.py # Automated audit report generation
â”œâ”€â”€ scripts/                  # Deployment scripts
â”‚   â”œâ”€â”€ start.sh              # Quick start
â”‚   â”œâ”€â”€ start_test.sh         # Long-running stability test
â”‚   â””â”€â”€ install_services.sh   # systemd service installer
â”œâ”€â”€ configs/                  # Configuration files
â”‚   â”œâ”€â”€ json.gbnf             # Grammar for constrained JSON decoding
â”‚   â”œâ”€â”€ bytetrack_stable.yaml # ByteTrack tracker configuration
â”‚   â”œâ”€â”€ zones.yaml            # Zone definitions
â”‚   â””â”€â”€ proximity_config.yaml # Proximity detection parameters
â”œâ”€â”€ services/                 # systemd service files
â”‚   â”œâ”€â”€ edge-runner.service   # Main application service
â”‚   â””â”€â”€ llama-server.service  # VLM inference server
â”œâ”€â”€ tests/                    # Test suite
â””â”€â”€ requirements.txt
```

---

## Hardware Requirements

| Component | Specification |
|-----------|--------------|
| **Device** | NVIDIA Jetson Orin Nano 8GB |
| **JetPack** | 6.0+ (Ubuntu 22.04, CUDA 12.x) |
| **Power** | MAXN mode (25W) |
| **Camera** | USB3 webcam (720p+ @ 30fps) |
| **Storage** | 32GB+ NVMe recommended |

---

## Quick Start

```bash
# 1. Clone and setup
git clone https://github.com/nosis-ai/edge-runner.git
cd edge-runner
python -m venv venv && source venv/bin/activate
pip install -r requirements.txt

# 2. Download models (YOLO + VLM)
# YOLO: Export via ultralytics (see docs)
# VLM: Download GGUF model for llama.cpp

# 3. Start VLM service
sudo systemctl start llama-server

# 4. Run EdgeRunner
python -m src.main

# 5. Open dashboard
# http://<DEVICE_IP>:8090/dashboard
```

---

## Modules

### Detection Pipeline (`src/cv_loop.py`)
- YOLO TensorRT inference with FP16 quantization
- ByteTrack multi-object tracking for temporal consistency
- Configurable ROI filtering and exclusion zones

### VLM Decision Engine (`src/vlm_client.py`, `src/machina_schema.py`)
- Async VLM inference via llama.cpp HTTP API
- GBNF grammar-constrained JSON output
- Pydantic v2 decision contracts with runtime validation
- Automatic fallback to safe state on timeout

### Safety Modules
- **Proximity Detection** (`src/proximity_detector.py`): Ground-plane calibrated distance estimation
- **Time-to-Contact** (`src/ttc.py`): Predictive collision timing
- **BBox Smoother** (`src/bbox_smoother.py`): Exponential moving average for flicker reduction
- **Alert Manager** (`src/alert_manager.py`): Event-driven alerting with configurable thresholds

### Observability
- Real-time MJPEG video stream
- Server-Sent Events for live metrics
- SLA tracking: FPS P50/P95, VLM latency, fallback rate
- Automated stability test suite

---

## License

Apache 2.0

---

*Built for a world where AI thinks locally.* ğŸ¤–
