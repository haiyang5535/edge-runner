#!/usr/bin/env python3
"""
Machina Decision Contract Schema - Pydantic v2
=================================================

Single source of truth for:
1. Constrained decoding (export ‚Üí JSON Schema ‚Üí llama.cpp GBNF)
2. Runtime validation (Pydantic model_validate)
3. API documentation
4. Audit logging

Core innovation: decode-time guaranteed contracts, not prompt engineering.
"""

from enum import Enum
from typing import Literal
from pydantic import BaseModel, Field, ConfigDict
import time
import uuid


# ============================================================
# Enums (constrained at decode time by llama.cpp grammar)
# ============================================================

class TrackStatus(str, Enum):
    """Fast path tracking status"""
    TRACKING = "TRACKING"
    LOST = "LOST"
    SEARCHING = "SEARCHING"


class Decision(str, Enum):
    """Slow path VLM decision outcome"""
    SAFE = "SAFE"
    SUSPICIOUS = "SUSPICIOUS"
    BREACH = "BREACH"
    UNKNOWN = "UNKNOWN"


class ReasonCode(str, Enum):
    """Reason codes explaining the decision"""
    AUTHORIZED = "AUTHORIZED"
    UNAUTHORIZED_OBJECT = "UNAUTHORIZED_OBJECT"
    LOW_CONFIDENCE = "LOW_CONFIDENCE"
    INSUFFICIENT_EVIDENCE = "INSUFFICIENT_EVIDENCE"
    TARGET_LOST = "TARGET_LOST"


class ActionType(str, Enum):
    """Control action to execute"""
    FOLLOW = "FOLLOW"
    STOP = "STOP"
    ALERT = "ALERT"
    SEARCH = "SEARCH"
    LOG_ONLY = "LOG_ONLY"


class Priority(str, Enum):
    """Action priority level"""
    LOW = "LOW"
    MEDIUM = "MEDIUM"
    HIGH = "HIGH"


# ============================================================
# Machina Contract Sub-Models
# ============================================================

class FastPath(BaseModel):
    """
    Fast loop perception data.
    
    NOTE: This is injected by the CV pipeline, NOT generated by VLM.
    VLM only sees this as context in the prompt.
    """
    model_config = ConfigDict(populate_by_name=True)
    
    track_id: int = Field(default=-1, description="ByteTrack ID, -1 if no target")
    class_name: Literal["person"] = Field(default="person", alias="class")
    bbox_xyxy: list[int] = Field(
        default_factory=lambda: [0, 0, 0, 0],
        min_length=4,
        max_length=4,
        description="Bounding box [x1, y1, x2, y2]"
    )
    track_status: TrackStatus = Field(default=TrackStatus.SEARCHING)


class Policy(BaseModel):
    """
    Active policy context.
    
    Defines what the VLM should watch for in this deployment.
    """
    mode: Literal["SAFETY_PATROL"] = Field(default="SAFETY_PATROL")
    rules: list[str] = Field(
        default_factory=lambda: [
            "PHONE_VISIBLE_IN_RESTRICTED_AREA",
            "RED_OBJECT_IS_HAZARDOUS"
        ]
    )


class Evidence(BaseModel):
    """VLM observation evidence - what was actually seen"""
    objects_seen: list[str] = Field(
        default_factory=lambda: ["unknown"],
        description="Objects observed in the image"
    )
    notes: str = Field(
        default="",
        max_length=200,
        description="Brief factual description, no speculation"
    )


class SlowPath(BaseModel):
    """
    Slow loop VLM decision - the semantic understanding.
    
    This is the core VLM output that gets constrained by grammar.
    """
    decision: Decision = Field(
        default=Decision.UNKNOWN,
        description="Overall safety assessment"
    )
    reason_codes: list[ReasonCode] = Field(
        default_factory=lambda: [ReasonCode.INSUFFICIENT_EVIDENCE],
        description="Codes explaining the decision"
    )
    confidence: float = Field(
        default=0.0,
        ge=0.0,
        le=1.0,
        description="Confidence in the decision (0.0-1.0)"
    )
    evidence: Evidence = Field(default_factory=Evidence)


class Action(BaseModel):
    """Resulting action command for the control system"""
    type: ActionType = Field(
        default=ActionType.LOG_ONLY,
        description="Action to execute"
    )
    priority: Priority = Field(
        default=Priority.LOW,
        description="Action priority"
    )


class Runtime(BaseModel):
    """Runtime metadata for monitoring and audit"""
    vlm_latency_ms: int = Field(
        default=0,
        description="VLM inference time in milliseconds"
    )
    json_parse_ok: bool = Field(
        default=True,
        description="Whether JSON parsing succeeded"
    )
    fallback_used: bool = Field(
        default=False,
        description="Whether SAFE_FALLBACK was used"
    )


# ============================================================
# Main Contract Model
# ============================================================

class MachinaDecision(BaseModel):
    """
    Machina Decision Contract - Complete output structure.
    
    This is the SINGLE SOURCE OF TRUTH for:
    1. llama.cpp constrained decoding (via .model_json_schema())
    2. Runtime validation (via .model_validate())
    3. Audit logging
    
    Usage:
        # For constrained decoding (give to llama.cpp)
        schema = get_vlm_json_schema()
        
        # For validation
        decision = MachinaDecision.model_validate(json_data)
        
        # For safe fallback
        fallback = SAFE_FALLBACK
    """
    schema_version: Literal["1.0"] = Field(default="1.0")
    decision_id: str = Field(
        default_factory=lambda: str(uuid.uuid4())[:8],
        description="Unique ID for this decision (for audit trail)"
    )
    timestamp_mono_ms: int = Field(
        default_factory=lambda: int(time.monotonic() * 1000),
        description="Monotonic timestamp (avoids NTP drift)"
    )
    
    fast_path: FastPath = Field(default_factory=FastPath)
    policy: Policy = Field(default_factory=Policy)
    slow_path: SlowPath = Field(default_factory=SlowPath)
    action: Action = Field(default_factory=Action)
    runtime: Runtime = Field(default_factory=Runtime)


# ============================================================
# VLM-Specific Schema (for constrained decoding)
# ============================================================

class VLMOutput(BaseModel):
    """
    Minimal schema for VLM output.
    
    VLM only generates slow_path + action.
    fast_path and runtime are injected by the CV pipeline.
    
    This keeps the VLM output small and focused.
    """
    slow_path: SlowPath
    action: Action


def get_vlm_json_schema() -> dict:
    """
    Export JSON Schema for llama.cpp constrained decoding.
    
    This schema is passed to llama-server's response_format.json_schema,
    which converts it to GBNF grammar for decode-time constraint.
    
    Returns:
        dict: JSON Schema for VLMOutput
    """
    return VLMOutput.model_json_schema()


def get_vlm_json_schema_compact() -> dict:
    """
    Get a compact, hand-crafted JSON schema optimized for minimal token usage.
    
    This inlines all definitions instead of using $refs, which significantly
    reduces token consumption in the llama.cpp context window.
    
    Returns:
        dict: Compact JSON Schema for VLM output
    """
    return {
        "type": "object",
        "properties": {
            "slow_path": {
                "type": "object",
                "properties": {
                    "decision": {
                        "type": "string",
                        "enum": ["SAFE", "SUSPICIOUS", "BREACH", "UNKNOWN"]
                    },
                    "confidence": {
                        "type": "number",
                        "minimum": 0,
                        "maximum": 1
                    },
                    "evidence": {
                        "type": "object",
                        "properties": {
                            "objects_seen": {
                                "type": "array",
                                "items": {"type": "string"}
                            },
                            "notes": {"type": "string"}
                        },
                        "required": ["objects_seen", "notes"]
                    }
                },
                "required": ["decision", "confidence", "evidence"]
            },
            "action": {
                "type": "object",
                "properties": {
                    "type": {
                        "type": "string",
                        "enum": ["FOLLOW", "STOP", "ALERT", "SEARCH", "LOG_ONLY"]
                    }
                },
                "required": ["type"]
            }
        },
        "required": ["slow_path", "action"]
    }


def get_vlm_json_schema_for_response_format(compact: bool = True) -> dict:
    """
    Get the schema in the format expected by llama.cpp response_format.
    
    Args:
        compact: If True, use the compact hand-crafted schema (recommended)
    
    Returns:
        dict: Ready to use as response_format value
    """
    schema = get_vlm_json_schema_compact() if compact else get_vlm_json_schema()
    return {
        "type": "json_schema",
        "json_schema": {
            "name": "machina_decision",
            "strict": True,
            "schema": schema
        }
    }


# ============================================================
# Safe Fallback (guaranteed valid)
# ============================================================

SAFE_FALLBACK = MachinaDecision(
    decision_id="FALLBACK",
    slow_path=SlowPath(
        decision=Decision.UNKNOWN,
        reason_codes=[ReasonCode.INSUFFICIENT_EVIDENCE],
        confidence=0.0,
        evidence=Evidence(
            objects_seen=["unknown"],
            notes="Safe fallback - VLM response was invalid or timed out"
        )
    ),
    action=Action(
        type=ActionType.LOG_ONLY,
        priority=Priority.LOW
    ),
    runtime=Runtime(
        json_parse_ok=False,
        fallback_used=True
    )
)


def build_full_decision(
    vlm_output: dict,
    track_id: int = -1,
    bbox: list[int] = None,
    track_status: str = "SEARCHING",
    vlm_latency_ms: int = 0
) -> MachinaDecision:
    """
    Build a complete MachinaDecision from VLM output + CV data.
    
    Handles both:
    - Full Pydantic schema output (with reason_codes, priority)
    - Compact schema output (minimal fields only)
    
    Args:
        vlm_output: Parsed JSON from VLM (should have slow_path + action)
        track_id: ByteTrack ID from CV pipeline
        bbox: Bounding box [x1, y1, x2, y2] from CV pipeline
        track_status: Current tracking status
        vlm_latency_ms: VLM inference time
    
    Returns:
        MachinaDecision: Complete decision contract
    """
    if bbox is None:
        bbox = [0, 0, 0, 0]
    
    try:
        # Extract slow_path and action from VLM output
        sp = vlm_output.get("slow_path", {})
        act = vlm_output.get("action", {})
        ev = sp.get("evidence", {})
        
        # Map decision to reason_code (for compact schema that omits reason_codes)
        decision_str = sp.get("decision", "UNKNOWN")
        default_reason = {
            "SAFE": ReasonCode.AUTHORIZED,
            "SUSPICIOUS": ReasonCode.UNAUTHORIZED_OBJECT,
            "BREACH": ReasonCode.UNAUTHORIZED_OBJECT,
            "UNKNOWN": ReasonCode.INSUFFICIENT_EVIDENCE
        }.get(decision_str, ReasonCode.INSUFFICIENT_EVIDENCE)
        
        # Build SlowPath with defaults for optional fields
        slow_path = SlowPath(
            decision=Decision(decision_str),
            reason_codes=sp.get("reason_codes", [default_reason]),
            confidence=float(sp.get("confidence", 0.0)),
            evidence=Evidence(
                objects_seen=ev.get("objects_seen", ["unknown"]),
                notes=ev.get("notes", "")[:200]  # Truncate to max length
            )
        )
        
        # Build Action with defaults for optional fields
        action = Action(
            type=ActionType(act.get("type", "LOG_ONLY")),
            priority=Priority(act.get("priority", "MEDIUM"))
        )
        
        # Build complete decision
        return MachinaDecision(
            fast_path=FastPath(
                track_id=track_id,
                bbox_xyxy=bbox,
                track_status=TrackStatus(track_status)
            ),
            slow_path=slow_path,
            action=action,
            runtime=Runtime(
                vlm_latency_ms=vlm_latency_ms,
                json_parse_ok=True,
                fallback_used=False
            )
        )
    except Exception as e:
        # Validation failed - return safe fallback
        fallback = SAFE_FALLBACK.model_copy(deep=True)
        fallback.fast_path.track_id = track_id
        fallback.fast_path.bbox_xyxy = bbox
        fallback.fast_path.track_status = TrackStatus(track_status)
        fallback.runtime.vlm_latency_ms = vlm_latency_ms
        fallback.slow_path.evidence.notes = f"Validation error: {str(e)[:100]}"
        return fallback


# ============================================================
# Test / Demo
# ============================================================

if __name__ == "__main__":
    import json
    
    print("=" * 60)
    print("Machina Decision Contract Schema")
    print("=" * 60)
    
    # 1. Show VLM JSON Schema (for constrained decoding)
    print("\nüìã VLM JSON Schema (for llama.cpp constrained decoding):")
    print("-" * 60)
    schema = get_vlm_json_schema()
    print(json.dumps(schema, indent=2))
    
    # 2. Show response_format structure
    print("\nüìã Response Format (for llama.cpp API):")
    print("-" * 60)
    rf = get_vlm_json_schema_for_response_format()
    print(json.dumps(rf, indent=2)[:500] + "...")
    
    # 3. Show safe fallback
    print("\nüõ°Ô∏è Safe Fallback:")
    print("-" * 60)
    print(SAFE_FALLBACK.model_dump_json(indent=2))
    
    # 4. Test build_full_decision with valid input
    print("\n‚úÖ Test: Valid VLM output")
    print("-" * 60)
    valid_vlm_output = {
        "slow_path": {
            "decision": "SAFE",
            "reason_codes": ["AUTHORIZED"],
            "confidence": 0.87,
            "evidence": {
                "objects_seen": ["person"],
                "notes": "Single person standing, no restricted items visible"
            }
        },
        "action": {
            "type": "FOLLOW",
            "priority": "MEDIUM"
        }
    }
    decision = build_full_decision(
        vlm_output=valid_vlm_output,
        track_id=42,
        bbox=[100, 150, 300, 450],
        track_status="TRACKING",
        vlm_latency_ms=2850
    )
    print(decision.model_dump_json(indent=2))
    
    # 5. Test build_full_decision with invalid input
    print("\n‚ö†Ô∏è Test: Invalid VLM output (triggers fallback)")
    print("-" * 60)
    invalid_vlm_output = {"garbage": "data"}
    fallback_decision = build_full_decision(
        vlm_output=invalid_vlm_output,
        track_id=42,
        bbox=[100, 150, 300, 450],
        vlm_latency_ms=3000
    )
    print(f"Fallback used: {fallback_decision.runtime.fallback_used}")
    print(f"Decision: {fallback_decision.slow_path.decision}")
    print(f"Action: {fallback_decision.action.type}")
    
    print("\n" + "=" * 60)
    print("Schema test complete!")
